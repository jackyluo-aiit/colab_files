{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackyluo-aiit/colab_files/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6JQhO6qjTsU",
        "colab_type": "code",
        "outputId": "cc9c1d41-5bab-4236-fc51-621849e99d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QzRMF9fju9i",
        "colab_type": "code",
        "outputId": "fb4e0e98-cb9f-4a71-90a7-bb6b18df1678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "!ls\n",
        "import os\n",
        "os.chdir('drive/Dataset')\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  drive  sample_data\n",
            "doc2vec_model\n",
            "drive\n",
            "epoch10_mincount500_window5_dm_alpha025_worker7_minalpha001_model_v2\n",
            "epoch10_mincount500_window5_dm_alpha025_worker7_minalpha001_model_v2.docvecs.vectors_docs.npy\n",
            "LSH_data.txt\n",
            "preprocessed_data.csv\n",
            "sentiment_cleaned.csv\n",
            "stream_data.txt\n",
            "testSet2.txt\n",
            "testSet.txt\n",
            "training.1600000.processed.noemoticon.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoRDRXS_eOrH",
        "colab_type": "code",
        "outputId": "45e62ce7-71a4-4b0e-d77d-550beabd8e9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "!pip install tweet-preprocessor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tweet-preprocessor\n",
            "  Downloading https://files.pythonhosted.org/packages/2a/f8/810ec35c31cca89bc4f1a02c14b042b9ec6c19dd21f7ef1876874ef069a6/tweet-preprocessor-0.5.0.tar.gz\n",
            "Building wheels for collected packages: tweet-preprocessor\n",
            "  Building wheel for tweet-preprocessor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.5.0-cp36-none-any.whl size=7947 sha256=6b8593506dfe13464019623094bde4eb53ff5e7d78519a2493f6cf8e0a16d882\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/27/cc/49938e98a2470802ebdefae9d2b3f524768e970c1ebbe2dc4a\n",
            "Successfully built tweet-preprocessor\n",
            "Installing collected packages: tweet-preprocessor\n",
            "Successfully installed tweet-preprocessor-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-2sCkkSNi-5",
        "colab_type": "code",
        "outputId": "759b5e78-0f5f-4025-e0b3-d1675e6da9c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import preprocessor as p\n",
        "from nltk.stem.porter import *\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import csv\n",
        "\n",
        "'''\n",
        "This file is to select the data with polarity are '0' and '4';\n",
        "Then clean the data, which only remains the words and punctuations.\n",
        "'''\n",
        "# data = pd.read_csv(\n",
        "#     \"training.1600000.processed.noemoticon.csv\",\n",
        "#     encoding=\"ISO-8859-1\")\n",
        "\n",
        "# data.columns = ['label', '', '', '', '', 'content']\n",
        "# sel_data = data[(data['label'] == 0) | (data['label'] == 4)]\n",
        "# print(\"selected 0 and 4 data:\", sel_data)\n",
        "# sel_content = sel_data['content']\n",
        "# content = []\n",
        "\n",
        "# for line in range(len(sel_content)):\n",
        "#     clean_line = p.clean(sel_content[line])\n",
        "#     # print(clean_line)\n",
        "#     content.append(clean_line)\n",
        "\n",
        "# # tfidf_vectorizer = TfidfVectorizer()\n",
        "# # x = tfidf_vectorizer.fit_transform(content)\n",
        "# # print(tfidf_vectorizer.get_feature_names())\n",
        "# # # print(x.toarray().shape())\n",
        "# #\n",
        "# # # print(x.shape())\n",
        "# # # print(data[['label', 'content']])\n",
        "# # vector = pd.DataFrame(x.toarray())\n",
        "# # print(vector)\n",
        "\n",
        "\n",
        "# new_data = sel_data[['label']]\n",
        "# new_data.insert(1, 'content', content)\n",
        "# print(\"new data:\", new_data.head)\n",
        "\n",
        "\n",
        "\n",
        "# def remove_pattern(input_txt, pattern):\n",
        "#     r = re.findall(pattern, input_txt)\n",
        "#     for i in r:\n",
        "#         input_txt = re.sub(i, '', input_txt)\n",
        "\n",
        "#     return input_txt\n",
        "\n",
        "\n",
        "# new_data['clean_data'] = np.vectorize(remove_pattern)(new_data['content'], \"@[\\w]*\")\n",
        "# new_data['clean_data'] = new_data['clean_data'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
        "# new_data['clean_data'] = new_data['clean_data'].apply(lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
        "# tokenized_content = new_data['clean_data'].apply(lambda x: x.split())\n",
        "\n",
        "# stemmer = PorterStemmer()\n",
        "# tokenized_content = tokenized_content.apply(lambda x: [stemmer.stem(i) for i in x])\n",
        "# for i in range(len(tokenized_content)):\n",
        "#     tokenized_content[i] = ' '.join(tokenized_content[i])\n",
        "\n",
        "# new_data['clean_data'] = tokenized_content\n",
        "# print(\"after tokenized:\", new_data)\n",
        "# print(\"new data len:\", len(new_data))\n",
        "# unique_content = new_data.content.unique()\n",
        "# print(\"content len:\", new_data.content.count())\n",
        "# print(\"unique content len:\", len(unique_content))\n",
        "# unique_clean_data = new_data.clean_data.unique()\n",
        "# print(\"unique clean content len:\", len(unique_clean_data))\n",
        "\n",
        "# print(\"Empty content:\", new_data[new_data.content.isnull()])\n",
        "# print(\"Empty clean_content:\", new_data[new_data.clean_data.isnull()])\n",
        "\n",
        "# new_data.drop_duplicates(subset=['clean_data'], keep='first', inplace=True)\n",
        "# new_data.reset_index(drop=True, inplace=True)\n",
        "# new_data['clean_data_length'] = new_data['clean_data'].apply(len)\n",
        "# print(new_data.info())\n",
        "# print(\"new data:\\n\", new_data.loc[:,('label','content','clean_data')])\n",
        "# new_data.to_csv(\"preprocessed_data.csv\")\n",
        "new_data = pd.read_csv(\"preprocessed_data.csv\")\n",
        "print(new_data.loc[:, ('clean_data')])\n",
        "\n",
        "tf_idf_vect = CountVectorizer(analyzer='word',ngram_range=(1,1),stop_words='english', min_df = 0.0001)\n",
        "tf_idf_vect.fit(new_data['clean_data'].values.astype('U'))\n",
        "desc_matrix = tf_idf_vect.transform(new_data[\"clean_data\"].values.astype('U'))\n",
        "\n",
        "num_clusters = 2\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(desc_matrix)\n",
        "clusters = km.labels_.tolist()\n",
        "\n",
        "tweets = {'Tweet': new_data[\"clean_data\"].tolist(), 'cluster': clusters}\n",
        "cluster_frame = pd.DataFrame(tweets, index = [clusters])\n",
        "print(\"after clustering:\\n\",cluster_frame)\n",
        "\n",
        "label_dict = {i: np.where(km.labels_ == i)[0] for i in range(km.n_clusters)}\n",
        "print(\"cluster label set:\\n\", label_dict)\n",
        "cluster_fp_set = {}\n",
        "for key in label_dict.keys():\n",
        "  count = 0\n",
        "  cluster_set = label_dict[key]\n",
        "  for item in cluster_set:\n",
        "    if key != 0:\n",
        "      key = 4\n",
        "    if new_data.loc[item, ['label']].values != key:\n",
        "      count+=1\n",
        "    cluster_fp = count/len(cluster_set)\n",
        "    cluster_fp_set[key] = cluster_fp\n",
        "\n",
        "# result_frame = pd.DataFrame(cluster_fp_set, )\n",
        "\n",
        "print(\"fpp:\\n\",cluster_fp_set)\n",
        "with open('result_TFIDF.csv','wb') as f:\n",
        "    w = csv.writer(f)\n",
        "    w.writerows(cluster_fp_set.items())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0          upset that updat facebook text might result sc...\n",
            "1                  dive mani time ball manag save rest bound\n",
            "2                            whole bodi feel itchi like fire\n",
            "3                               behav here becaus over there\n",
            "4                                                 whole crew\n",
            "                                 ...                        \n",
            "1435443    yeah that doe work better than just wait just ...\n",
            "1435444                 just woke have school best feel ever\n",
            "1435445                 thewdb veri cool hear walt interview\n",
            "1435446                        readi your mojo makeov detail\n",
            "1435447          happi birthday alll time tupac amaru shakur\n",
            "Name: clean_data, Length: 1435448, dtype: object\n",
            "after clustering:\n",
            "                                                 Tweet  cluster\n",
            "1   upset that updat facebook text might result sc...        1\n",
            "1           dive mani time ball manag save rest bound        1\n",
            "1                     whole bodi feel itchi like fire        1\n",
            "1                        behav here becaus over there        1\n",
            "1                                          whole crew        1\n",
            "..                                                ...      ...\n",
            "1   yeah that doe work better than just wait just ...        1\n",
            "1                just woke have school best feel ever        1\n",
            "1                thewdb veri cool hear walt interview        1\n",
            "1                       readi your mojo makeov detail        1\n",
            "1         happi birthday alll time tupac amaru shakur        1\n",
            "\n",
            "[1435448 rows x 2 columns]\n",
            "cluster label set:\n",
            " {0: array([    101,     117,     180, ..., 1435400, 1435430, 1435436]), 1: array([      0,       1,       2, ..., 1435445, 1435446, 1435447])}\n",
            "fpp:\n",
            " {0: 0.6415376002985632, 4: 0.5105345319251796}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e4b57162c3f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fpp:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster_fp_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'result_TFIDF.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_fp_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
          ]
        }
      ]
    }
  ]
}